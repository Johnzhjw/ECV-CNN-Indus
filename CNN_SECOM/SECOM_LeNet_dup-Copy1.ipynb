{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file2array(path, delimiter):\n",
    "    recordlist = []\n",
    "    fp = open(path, 'r', encoding='utf-8')\n",
    "    content = fp.read()     # content现在是一行字符串，该字符串包含文件所有内容\n",
    "    fp.close()\n",
    "    rowlist = content.splitlines()  # 按行转换为一维表，splitlines默认参数是‘\\n’\n",
    "    # 逐行遍历\n",
    "    # 结果按分隔符分割为行向量\n",
    "    recordlist = [row.split(delimiter) for row in rowlist if row.strip()]\n",
    "    return np.array(recordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "def ndarray_zoom_scaling(label, new_h, new_w):\n",
    "    \"\"\"\n",
    "    Implement scaling for ndarray with scipy.ndimage.zoom\n",
    "    :param label: [H, W] or [H, W, C]\n",
    "    :return: label_new: [new_h, new_w] or [new_h, new_w, C]\n",
    "    Examples\n",
    "    --------\n",
    "    ori_arr = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]], dtype=np.int32)\n",
    "    new_arr = ndarray_zoom_scaling(ori_arr, new_h=8, new_w=10)\n",
    "    >> print(new_arr)\n",
    "    [[1 1 1 2 2 2 2 3 3 3]\n",
    "     [1 1 1 2 2 2 2 3 3 3]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [7 7 7 8 8 8 8 9 9 9]\n",
    "     [7 7 7 8 8 8 8 9 9 9]]\n",
    "    \"\"\"\n",
    "    scale_h = new_h / label.shape[0]\n",
    "    scale_w = new_w / label.shape[1]\n",
    "    if len(label.shape) == 2:\n",
    "        label_new = scipy.ndimage.zoom(label, zoom=[scale_h, scale_w], order=0)\n",
    "    else:\n",
    "        label_new = scipy.ndimage.zoom(label, zoom=[scale_h, scale_w, 1], order=0)\n",
    "    return label_new\n",
    "\n",
    "\n",
    "def ndarray_nearest_neighbour_scaling(label, new_h, new_w):\n",
    "    \"\"\"\n",
    "    Implement nearest neighbour scaling for ndarray\n",
    "    :param label: [H, W] or [H, W, C]\n",
    "    :return: label_new: [new_h, new_w] or [new_h, new_w, C]\n",
    "    Examples\n",
    "    --------\n",
    "    ori_arr = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]], dtype=np.int32)\n",
    "    new_arr = ndarray_nearest_neighbour_scaling(ori_arr, new_h=8, new_w=10)\n",
    "    >> print(new_arr)\n",
    "    [[1 1 1 1 2 2 2 3 3 3]\n",
    "     [1 1 1 1 2 2 2 3 3 3]\n",
    "     [1 1 1 1 2 2 2 3 3 3]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [7 7 7 7 8 8 8 9 9 9]\n",
    "     [7 7 7 7 8 8 8 9 9 9]]\n",
    "    \"\"\"\n",
    "#    if len(label.shape) == 2:\n",
    "#        label_new = np.zeros([new_h, new_w], dtype=label.dtype)\n",
    "#    else:\n",
    "#        label_new = np.zeros([new_h, new_w, label.shape[2]], dtype=label.dtype)\n",
    "    label_new = np.zeros([new_h, new_w, label.shape[2]], dtype=label.dtype)\n",
    "\n",
    "    scale_h = new_h / label.shape[0]\n",
    "    scale_w = new_w / label.shape[1]\n",
    "\n",
    "    y_pos = np.arange(new_h)\n",
    "    x_pos = np.arange(new_w)\n",
    "    y_pos = np.floor(y_pos / scale_h).astype(np.int32)\n",
    "    x_pos = np.floor(x_pos / scale_w).astype(np.int32)\n",
    "\n",
    "    y_pos = y_pos.reshape(y_pos.shape[0], 1)\n",
    "    y_pos = np.tile(y_pos, (1, new_w))\n",
    "    x_pos = np.tile(x_pos, (new_h, 1))\n",
    "    assert y_pos.shape == x_pos.shape\n",
    "\n",
    "    label_new[:, :] = label[y_pos[:, :], x_pos[:, :]]\n",
    "    return label_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DataSet(iFold):\n",
    "    \n",
    "    train_path =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_samples_train_F%d\") % (iFold, iFold)    \n",
    "    test_path  =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_samples_test_F%d\") % (iFold, iFold)\n",
    "    #\n",
    "    data_train = file2array(train_path, \" \")\n",
    "    nrow_train = data_train.shape[0]\n",
    "    ncol_train = data_train.shape[1]\n",
    "    #\n",
    "    data_test  = file2array(test_path, \" \")\n",
    "    nrow_test  = data_test.shape[0]\n",
    "    ncol_test  = data_test.shape[1]\n",
    "    #\n",
    "    if ncol_train != ncol_test:\n",
    "        print('Numbers of features are not consistant.')\n",
    "    if nrow_train + nrow_test != 1567:\n",
    "        print('Numbers of samples are not consistant.')\n",
    "    #\n",
    "    train_path =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_labels_train_F%d\") % (iFold, iFold)    \n",
    "    test_path  =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_labels_test_F%d\") % (iFold, iFold)\n",
    "    #\n",
    "    lab_train = file2array(train_path, \" \")\n",
    "    #\n",
    "    lab_test  = file2array(test_path, \" \")\n",
    "    #\n",
    "    num_feature = ncol_train\n",
    "    side_len  = math.ceil(math.sqrt(num_feature))\n",
    "    num_class = 2\n",
    "    \n",
    "    tmp_cnt_per_class = np.zeros(num_class)\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        tmp_cnt_per_class[(int)(tmp)] = tmp_cnt_per_class[(int)(tmp)] + 1\n",
    "        \n",
    "    tmp_max = tmp_cnt_per_class[0]\n",
    "    tmp_ind = 0\n",
    "    for index in range(num_class):\n",
    "        if tmp_cnt_per_class[index] > tmp_max:\n",
    "            tmp_max = tmp_cnt_per_class[index]\n",
    "            tmp_ind = index\n",
    "            \n",
    "    num_dup = np.zeros(num_class)\n",
    "    for index in range(num_class):\n",
    "        num_dup[index] = math.floor(tmp_cnt_per_class[tmp_ind]/tmp_cnt_per_class[index])\n",
    "        \n",
    "    nrow_train_dup = 0\n",
    "    for index in range(num_class):\n",
    "        nrow_train_dup = nrow_train_dup + tmp_cnt_per_class[index] * num_dup[index]\n",
    "        \n",
    "    nrow_train_dup = (int)(nrow_train_dup)\n",
    "    \n",
    "    X_train = np.empty((nrow_train_dup, side_len, side_len, 1))\n",
    "    Y_train = np.empty((nrow_train_dup, num_class))\n",
    "\n",
    "    sam_count = 0\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        nDup = num_dup[(int)(tmp)]\n",
    "        nDup = (int)(nDup)\n",
    "        for iSam in range(nDup):\n",
    "            tmp_cnt = 0\n",
    "            for i in range(side_len):\n",
    "                for j in range(side_len):\n",
    "                    if tmp_cnt < num_feature:\n",
    "                        X_train[sam_count,i,j,0] = data_train[index,tmp_cnt]\n",
    "                    else:\n",
    "                        X_train[sam_count,i,j,0] = 0.5\n",
    "                    tmp_cnt = tmp_cnt + 1\n",
    "            sam_count = sam_count + 1\n",
    "\n",
    "    sam_count = 0\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        nDup = num_dup[(int)(tmp)]\n",
    "        nDup = (int)(nDup)\n",
    "        for iSam in range(nDup):\n",
    "            tmp_v = np.zeros(num_class)\n",
    "            tmp_v[(int)(tmp)] = 1\n",
    "            Y_train[sam_count] = tmp_v\n",
    "            sam_count = sam_count + 1\n",
    "    \n",
    "    X_test  = np.empty((nrow_test, side_len, side_len, 1))\n",
    "    Y_test  = np.empty((nrow_test, num_class))\n",
    "\n",
    "    for index in range(nrow_test):\n",
    "        tmp_cnt = 0\n",
    "        for i in range(side_len):\n",
    "            for j in range(side_len):\n",
    "                if tmp_cnt < num_feature:\n",
    "                    X_test[index,i,j,0] = data_test[index,tmp_cnt]\n",
    "                else:\n",
    "                    X_test[index,i,j,0] = 0.5\n",
    "                tmp_cnt = tmp_cnt + 1\n",
    "    \n",
    "    for index in range(nrow_test):\n",
    "        tmp = lab_test[index,0]\n",
    "        tmp_v = np.zeros(num_class)\n",
    "        tmp_v[(int)(tmp)] = 1\n",
    "        Y_train[index] = tmp_v\n",
    "    \n",
    "    index = [i for i in range(len(X_train))]\n",
    "    random.shuffle(index)\n",
    "    X_train = X_train[index]\n",
    "    Y_train = Y_train[index]\n",
    "    \n",
    "    index = [i for i in range(len(X_test))]\n",
    "    random.shuffle(index)\n",
    "    X_test = X_test[index]    \n",
    "    Y_test = Y_test[index]\n",
    "    \n",
    "    X_train_new = np.empty((nrow_train_dup, 224, 224, 3))\n",
    "    Y_train_new = Y_train\n",
    "    \n",
    "    for index in range(nrow_train_dup):\n",
    "        tmp_new = ndarray_nearest_neighbour_scaling(X_train[index],224,224)\n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                for c in range(3):\n",
    "                    X_train_new[index,i,j,c] = tmp_new[i,j,0]\n",
    "    \n",
    "    X_test_new = np.empty((nrow_test, 224, 224, 3))\n",
    "    Y_test_new = Y_test\n",
    "    \n",
    "    for index in range(nrow_test):\n",
    "        tmp_new = ndarray_nearest_neighbour_scaling(X_test[index],224,224)\n",
    "        for i in range(224):\n",
    "            for j in range(224):\n",
    "                for c in range(3):\n",
    "                    X_test_new[index,i,j,c] = tmp_new[i,j,0]\n",
    "\n",
    "    return X_train_new,Y_train,X_test_new,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_test,Y_test = DataSet(1)\n",
    "print('X_train shape : ',X_train.shape)\n",
    "print('Y_train shape : ',Y_train.shape)\n",
    "print('X_test shape : ',X_test.shape)\n",
    "print('Y_test shape : ',Y_test.shape)\n",
    "print(X_train[0])\n",
    "print(Y_train[0,1])\n",
    "print([i for i in range(len(X_train))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for fold in range(10):\n",
    "    iFold = fold + 1\n",
    "    X_train,Y_train,X_test,Y_test = DataSet(iFold)\n",
    "    print('X_train shape : ',X_train.shape)\n",
    "    print('Y_train shape : ',Y_train.shape)\n",
    "    print('X_test shape : ',X_test.shape)\n",
    "    print('Y_test shape : ',Y_test.shape)\n",
    "    print(X_train[0])\n",
    "    print(Y_train[0,1])\n",
    "\n",
    "    # # model\n",
    "    model = ResNet50(\n",
    "        weights=None,\n",
    "        classes=2\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # # train\n",
    "    model.fit(X_train, Y_train, epochs=1, batch_size=6)\n",
    "\n",
    "    # # evaluate\n",
    "    model.evaluate(X_test, Y_test, batch_size=32)\n",
    "    \n",
    "    # #\n",
    "    models.append(model)\n",
    "\n",
    "    # # save\n",
    "    save_path =(\"./my_resnet_model.h5_F%d\") % (iFold)\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # restore\n",
    "model = tf.keras.models.load_model('my_resnet_model.h5')\n",
    "\n",
    "# # test\n",
    "img_path = \"../my_nn/dataset/test/medicine/IMG_20190717_135408_BURST91.jpg\"\n",
    "img_path = \"../my_nn/dataset/test/glue/IMG_20190717_135425_BURST91.jpg\"\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "plt.imshow(img)\n",
    "img = image.img_to_array(img)/ 255.0\n",
    "img = np.expand_dims(img, axis=0)  # 为batch添加第四维\n",
    "\n",
    "print(model.predict(img))\n",
    "np.argmax(model.predict(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
