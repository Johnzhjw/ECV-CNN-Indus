{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from PIL import Image\n",
    "import random\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dropout, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2array(path, delimiter):\n",
    "    recordlist = []\n",
    "    fp = open(path, 'r', encoding='utf-8')\n",
    "    content = fp.read()     # content现在是一行字符串，该字符串包含文件所有内容\n",
    "    fp.close()\n",
    "    rowlist = content.splitlines()  # 按行转换为一维表，splitlines默认参数是‘\\n’\n",
    "    # 逐行遍历\n",
    "    # 结果按分隔符分割为行向量\n",
    "    recordlist = [row.split(delimiter) for row in rowlist if row.strip()]\n",
    "    return np.array(recordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "def ndarray_zoom_scaling(label, new_h, new_w):\n",
    "    \"\"\"\n",
    "    Implement scaling for ndarray with scipy.ndimage.zoom\n",
    "    :param label: [H, W] or [H, W, C]\n",
    "    :return: label_new: [new_h, new_w] or [new_h, new_w, C]\n",
    "    Examples\n",
    "    --------\n",
    "    ori_arr = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]], dtype=np.int32)\n",
    "    new_arr = ndarray_zoom_scaling(ori_arr, new_h=8, new_w=10)\n",
    "    >> print(new_arr)\n",
    "    [[1 1 1 2 2 2 2 3 3 3]\n",
    "     [1 1 1 2 2 2 2 3 3 3]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [7 7 7 8 8 8 8 9 9 9]\n",
    "     [7 7 7 8 8 8 8 9 9 9]]\n",
    "    \"\"\"\n",
    "    scale_h = new_h / label.shape[0]\n",
    "    scale_w = new_w / label.shape[1]\n",
    "    if len(label.shape) == 2:\n",
    "        label_new = scipy.ndimage.zoom(label, zoom=[scale_h, scale_w], order=0)\n",
    "    else:\n",
    "        label_new = scipy.ndimage.zoom(label, zoom=[scale_h, scale_w, 1], order=0)\n",
    "    return label_new\n",
    "\n",
    "\n",
    "def ndarray_nearest_neighbour_scaling(label, new_h, new_w):\n",
    "    \"\"\"\n",
    "    Implement nearest neighbour scaling for ndarray\n",
    "    :param label: [H, W] or [H, W, C]\n",
    "    :return: label_new: [new_h, new_w] or [new_h, new_w, C]\n",
    "    Examples\n",
    "    --------\n",
    "    ori_arr = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]], dtype=np.int32)\n",
    "    new_arr = ndarray_nearest_neighbour_scaling(ori_arr, new_h=8, new_w=10)\n",
    "    >> print(new_arr)\n",
    "    [[1 1 1 1 2 2 2 3 3 3]\n",
    "     [1 1 1 1 2 2 2 3 3 3]\n",
    "     [1 1 1 1 2 2 2 3 3 3]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [7 7 7 7 8 8 8 9 9 9]\n",
    "     [7 7 7 7 8 8 8 9 9 9]]\n",
    "    \"\"\"\n",
    "#    if len(label.shape) == 2:\n",
    "#        label_new = np.zeros([new_h, new_w], dtype=label.dtype)\n",
    "#    else:\n",
    "#        label_new = np.zeros([new_h, new_w, label.shape[2]], dtype=label.dtype)\n",
    "    label_new = np.zeros([new_h, new_w, label.shape[2]], dtype=label.dtype)\n",
    "\n",
    "    scale_h = new_h / label.shape[0]\n",
    "    scale_w = new_w / label.shape[1]\n",
    "\n",
    "    y_pos = np.arange(new_h)\n",
    "    x_pos = np.arange(new_w)\n",
    "    y_pos = np.floor(y_pos / scale_h).astype(np.int32)\n",
    "    x_pos = np.floor(x_pos / scale_w).astype(np.int32)\n",
    "\n",
    "    y_pos = y_pos.reshape(y_pos.shape[0], 1)\n",
    "    y_pos = np.tile(y_pos, (1, new_w))\n",
    "    x_pos = np.tile(x_pos, (new_h, 1))\n",
    "    assert y_pos.shape == x_pos.shape\n",
    "\n",
    "    label_new[:, :] = label[y_pos[:, :], x_pos[:, :]]\n",
    "    return label_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataSet(iFold,dup_tag,rand_tag):\n",
    "    #\n",
    "    train_path =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_samples_train_F%d\") % (iFold, iFold)    \n",
    "    test_path  =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_samples_test_F%d\") % (iFold, iFold)\n",
    "    #\n",
    "    data_train = file2array(train_path, \" \")\n",
    "    nrow_train = data_train.shape[0]\n",
    "    ncol_train = data_train.shape[1]\n",
    "    #\n",
    "    data_test  = file2array(test_path, \" \")\n",
    "    nrow_test  = data_test.shape[0]\n",
    "    ncol_test  = data_test.shape[1]\n",
    "    #\n",
    "    if ncol_train != ncol_test:\n",
    "        print('Numbers of features are not consistant.')\n",
    "    if nrow_train + nrow_test != 1567:\n",
    "        print('Numbers of samples are not consistant.')\n",
    "    #\n",
    "    train_path =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_labels_train_F%d\") % (iFold, iFold)    \n",
    "    test_path  =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/F%d/SECOM_labels_test_F%d\") % (iFold, iFold)\n",
    "    #\n",
    "    lab_train = file2array(train_path, \" \")\n",
    "    #\n",
    "    lab_test  = file2array(test_path, \" \")\n",
    "    #\n",
    "    num_feature = ncol_train\n",
    "    side_len  = math.ceil(math.sqrt(num_feature))\n",
    "    num_class = 2\n",
    "    #\n",
    "    mat_ind_path =(\"../../Research/Projects/DPNeuEvo/Data_all/Data_CNN_Indus/Mat_ind_F%d.txt\") % (iFold)\n",
    "    #\n",
    "    mat_ind = file2array(mat_ind_path, \"\\t\")\n",
    "    \n",
    "    if (int)(rand_tag):\n",
    "        tmp_vec = np.zeros(num_feature)\n",
    "        for index in range(num_feature):\n",
    "            r = (int)(index/side_len)\n",
    "            c = (int)(index%side_len)\n",
    "            tmp_vec[index] = int(mat_ind[r,c])\n",
    "        index = [i for i in range(num_feature)]\n",
    "        random.shuffle(index)\n",
    "        tmp_vec = tmp_vec[index]\n",
    "        for index in range(num_feature):\n",
    "            r = (int)(index/side_len)\n",
    "            c = (int)(index%side_len)\n",
    "            mat_ind[r,c] = int(tmp_vec[index])\n",
    "\n",
    "    tmp_cnt_per_class = np.zeros(num_class)\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        tmp_cnt_per_class[(int)(tmp)] = tmp_cnt_per_class[(int)(tmp)] + 1\n",
    "        \n",
    "    tmp_max = tmp_cnt_per_class[0]\n",
    "    tmp_ind = 0\n",
    "    for index in range(num_class):\n",
    "        if tmp_cnt_per_class[index] > tmp_max:\n",
    "            tmp_max = tmp_cnt_per_class[index]\n",
    "            tmp_ind = index\n",
    "            \n",
    "    num_dup = np.zeros(num_class)\n",
    "    for index in range(num_class):\n",
    "        if (int)(dup_tag):\n",
    "            num_dup[index] = math.floor(tmp_cnt_per_class[tmp_ind]/tmp_cnt_per_class[index])\n",
    "        else:\n",
    "            num_dup[index] = 1\n",
    "        \n",
    "    nrow_train_dup = 0\n",
    "    for index in range(num_class):\n",
    "        nrow_train_dup = nrow_train_dup + tmp_cnt_per_class[index] * num_dup[index]\n",
    "        \n",
    "    nrow_train_dup = (int)(nrow_train_dup)\n",
    "    \n",
    "    X_train = np.empty((nrow_train_dup, side_len, side_len, 1))\n",
    "    Y_train = np.empty((nrow_train_dup, num_class))\n",
    "\n",
    "    sam_count = 0\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        nDup = num_dup[(int)(tmp)]\n",
    "        nDup = (int)(nDup)\n",
    "        for iSam in range(nDup):\n",
    "            tmp_cnt = 0\n",
    "            for i in range(side_len):\n",
    "                for j in range(side_len):\n",
    "                    if tmp_cnt < num_feature:\n",
    "                        tmp_ind = mat_ind[i,j]\n",
    "                        X_train[sam_count,i,j,0] = data_train[index,(int)(tmp_ind)]\n",
    "                    else:\n",
    "                        X_train[sam_count,i,j,0] = 0.5\n",
    "                    tmp_cnt = tmp_cnt + 1\n",
    "            sam_count = sam_count + 1\n",
    "\n",
    "    sam_count = 0\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        nDup = num_dup[(int)(tmp)]\n",
    "        nDup = (int)(nDup)\n",
    "        for iSam in range(nDup):\n",
    "            tmp_v = np.zeros(num_class)\n",
    "            tmp_v[(int)(tmp)] = 1\n",
    "            Y_train[sam_count] = tmp_v\n",
    "            sam_count = sam_count + 1\n",
    "    \n",
    "    X_test  = np.empty((nrow_test, side_len, side_len, 1))\n",
    "    Y_test  = np.empty((nrow_test, num_class))\n",
    "\n",
    "    for index in range(nrow_test):\n",
    "        tmp_cnt = 0\n",
    "        for i in range(side_len):\n",
    "            for j in range(side_len):\n",
    "                if tmp_cnt < num_feature:\n",
    "                    tmp_ind = mat_ind[i,j]\n",
    "                    X_test[index,i,j,0] = data_test[index,(int)(tmp_ind)]\n",
    "                else:\n",
    "                    X_test[index,i,j,0] = 0.5\n",
    "                tmp_cnt = tmp_cnt + 1\n",
    "    \n",
    "    for index in range(nrow_test):\n",
    "        tmp = lab_test[index,0]\n",
    "        tmp_v = np.zeros(num_class)\n",
    "        tmp_v[(int)(tmp)] = 1\n",
    "        Y_test[index] = tmp_v\n",
    "    \n",
    "    index = [i for i in range(len(X_train))]\n",
    "    random.shuffle(index)\n",
    "    X_train = X_train[index]\n",
    "    Y_train = Y_train[index]\n",
    "    \n",
    "    index = [i for i in range(len(X_test))]\n",
    "    random.shuffle(index)\n",
    "    X_test = X_test[index]    \n",
    "    Y_test = Y_test[index]\n",
    "    \n",
    "    X_train_new = np.empty((nrow_train_dup, 224, 224, 3))\n",
    "    Y_train_new = Y_train\n",
    "    \n",
    "    for index in range(nrow_train_dup):\n",
    "        tmp_new = ndarray_nearest_neighbour_scaling(X_train[index],224,224)\n",
    "        for c in range(3):\n",
    "            X_train_new[index,:,:,c] = tmp_new[:,:,0]\n",
    "    \n",
    "    X_test_new = np.empty((nrow_test, 224, 224, 3))\n",
    "    Y_test_new = Y_test\n",
    "    \n",
    "    for index in range(nrow_test):\n",
    "        tmp_new = ndarray_nearest_neighbour_scaling(X_test[index],224,224)\n",
    "        for c in range(3):\n",
    "            X_test_new[index,:,:,c] = tmp_new[:,:,0]\n",
    "\n",
    "    return X_train_new,Y_train,X_test_new,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "def LeakyRelu(x, leak=0.01, name=\"LeakyRelu\"):\n",
    "   with tf.variable_scope(name):\n",
    "     f1 = 0.5 * (1 + leak)\n",
    "     f2 = 0.5 * (1 - leak)\n",
    "     return f1 * x + f2 * tf.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(channel,height,width,classes):\n",
    "    input_shape = (channel,height,width)\n",
    "    if K.image_data_format() == \"channels_last\":#确认输入维度,就是channel是在开头，还是结尾\n",
    "        input_shape = (height,width,channel)\n",
    "    model = Sequential()#顺序模型（keras中包括顺序模型和函数式API两种方式）\n",
    "    model.add(Conv2D(6,(3,3),padding=\"valid\",activation=LeakyRelu,input_shape=input_shape,name=\"conv1\"))\n",
    "    #model.add(Conv2D(6,(3,3),padding=\"valid\",input_shape=input_shape,name=\"conv1\"))\n",
    "    #model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(AveragePooling2D(pool_size=(2,2),strides=(2,2),name=\"pool1\"))\n",
    "    model.add(Conv2D(12,(3,3),padding=\"valid\",activation=LeakyRelu,name=\"conv2\",))\n",
    "    #model.add(Conv2D(12,(3,3),padding=\"valid\",name=\"conv2\",))\n",
    "    #model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(AveragePooling2D(pool_size=(2,2),strides=(2,2),name=\"pool2\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(192,activation=\"relu\",name=\"fc1\"))\n",
    "    model.add(Dense(classes,activation=\"softmax\",name=\"fc2\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(model,sam_data,lab_data):\n",
    "    num_class = 2\n",
    "\n",
    "    #\n",
    "    test_rows = len(lab_data)\n",
    "    test_prc = np.zeros(num_class)\n",
    "    test_rec = np.zeros(num_class)\n",
    "    test_Fva = np.zeros(num_class)\n",
    "    test_BER = np.zeros(num_class)\n",
    "    tmp_beta = 1\n",
    "    yhat_out    = model.predict(sam_data, batch_size=32, verbose=1)\n",
    "    yhat_label  = np.zeros(len(lab_data))\n",
    "    true_labels = np.zeros(len(lab_data))\n",
    "    for index in range(len(lab_data)):\n",
    "        tmp_vec = lab_data[index]\n",
    "        tmp_max = tmp_vec[0]\n",
    "        tmp_ind = 0\n",
    "        for i in range(num_class):\n",
    "            if tmp_max < tmp_vec[i]:\n",
    "                tmp_max = tmp_vec[i]\n",
    "                tmp_ind = i\n",
    "        true_labels[index] = tmp_ind\n",
    "        #\n",
    "        tmp_vec = yhat_out[index]\n",
    "        tmp_max = tmp_vec[0]\n",
    "        tmp_ind = 0\n",
    "        for i in range(num_class):\n",
    "            if tmp_max < tmp_vec[i]:\n",
    "                tmp_max = tmp_vec[i]\n",
    "                tmp_ind = i\n",
    "        yhat_label[index] = tmp_ind\n",
    "\n",
    "    test_N_TP = np.zeros(num_class)\n",
    "    test_N_FP = np.zeros(num_class)\n",
    "    test_N_TN = np.zeros(num_class)\n",
    "    test_N_FN = np.zeros(num_class)\n",
    "    for i in range(test_rows):\n",
    "        cur_label = yhat_label[i]\n",
    "        true_label = true_labels[i]\n",
    "        for j in range(num_class):\n",
    "            if j == cur_label:\n",
    "                if j == true_label:\n",
    "                    test_N_TP[j]=test_N_TP[j]+1\n",
    "                else:\n",
    "                    test_N_FP[j]=test_N_FP[j]+1\n",
    "            else:\n",
    "                if j == true_label:\n",
    "                    test_N_FN[j]=test_N_FN[j]+1\n",
    "                else:\n",
    "                    test_N_TN[j]=test_N_TN[j]+1\n",
    "\n",
    "    for i in range(num_class):\n",
    "        if test_N_TP[i] > 0:\n",
    "            test_prc[i] = test_N_TP[i] / (test_N_TP[i] + test_N_FP[i])\n",
    "        else:\n",
    "            test_prc[i] = 0\n",
    "\n",
    "        test_rec[i]= test_N_TP[i] / (test_N_TP[i] + test_N_FN[i])\n",
    "        test_Fva[i]=(1 + tmp_beta*tmp_beta)*test_rec[i]*test_prc[i]/\\\n",
    "                      (tmp_beta*tmp_beta*(test_rec[i]+test_prc[i]))\n",
    "        test_BER[i]= test_N_FN[i] / (test_N_TP[i] + test_N_FN[i])\n",
    "        \n",
    "    return test_N_TP,test_N_FP,test_N_TN,test_N_FN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (1410, 224, 224, 3)\n",
      "Y_train shape :  (1410, 2)\n",
      "X_test shape :  (157, 224, 224, 3)\n",
      "Y_test shape :  (157, 2)\n",
      "Epoch 1/10\n",
      "1410/1410 [==============================] - 2779s 2s/step - loss: 0.3709 - acc: 0.9177\n",
      "Epoch 2/10\n",
      "1410/1410 [==============================] - 2649s 2s/step - loss: 0.2839 - acc: 0.9305\n",
      "Epoch 3/10\n",
      "1410/1410 [==============================] - 1988s 1s/step - loss: 0.2651 - acc: 0.9291\n",
      "Epoch 4/10\n",
      "1410/1410 [==============================] - 1844s 1s/step - loss: 0.2620 - acc: 0.9298\n",
      "Epoch 5/10\n",
      "1410/1410 [==============================] - 1824s 1s/step - loss: 0.2276 - acc: 0.9305\n",
      "Epoch 6/10\n",
      "1410/1410 [==============================] - 1847s 1s/step - loss: 0.2129 - acc: 0.9362\n",
      "Epoch 7/10\n",
      "1410/1410 [==============================] - 1829s 1s/step - loss: 0.1895 - acc: 0.9418\n",
      "Epoch 8/10\n",
      "1410/1410 [==============================] - 1851s 1s/step - loss: 0.1869 - acc: 0.9376\n",
      "Epoch 9/10\n",
      "1410/1410 [==============================] - 1850s 1s/step - loss: 0.1781 - acc: 0.9411\n",
      "Epoch 10/10\n",
      "1410/1410 [==============================] - 1832s 1s/step - loss: 0.1853 - acc: 0.9376\n",
      "1410/1410 [==============================] - 610s 433ms/step\n",
      "157/157 [==============================] - 66s 422ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\toolkits.win\\Anaconda3-4.4.0\\envs\\dlwin36-CPU\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (1410, 224, 224, 3)\n",
      "Y_train shape :  (1410, 2)\n",
      "X_test shape :  (157, 224, 224, 3)\n",
      "Y_test shape :  (157, 2)\n",
      "Epoch 1/10\n",
      "1410/1410 [==============================] - 1828s 1s/step - loss: 0.3603 - acc: 0.9177\n",
      "Epoch 2/10\n",
      "1410/1410 [==============================] - 1817s 1s/step - loss: 0.2686 - acc: 0.9291\n",
      "Epoch 3/10\n",
      "1410/1410 [==============================] - 1815s 1s/step - loss: 0.2336 - acc: 0.9340\n",
      "Epoch 4/10\n",
      "1410/1410 [==============================] - 1810s 1s/step - loss: 0.2305 - acc: 0.9312\n",
      "Epoch 5/10\n",
      "1410/1410 [==============================] - 1821s 1s/step - loss: 0.2629 - acc: 0.9262\n",
      "Epoch 6/10\n",
      "1410/1410 [==============================] - 1817s 1s/step - loss: 0.2203 - acc: 0.9340\n",
      "Epoch 7/10\n",
      "1410/1410 [==============================] - 1808s 1s/step - loss: 0.1969 - acc: 0.9326\n",
      "Epoch 8/10\n",
      "1410/1410 [==============================] - 1911s 1s/step - loss: 0.1872 - acc: 0.9362\n",
      "Epoch 9/10\n",
      "1410/1410 [==============================] - 1898s 1s/step - loss: 0.1555 - acc: 0.9447\n",
      "Epoch 10/10\n",
      "1410/1410 [==============================] - 2088s 1s/step - loss: 0.1633 - acc: 0.9383\n",
      "1410/1410 [==============================] - 722s 512ms/step\n",
      "157/157 [==============================] - 82s 524ms/step\n",
      "X_train shape :  (1410, 224, 224, 3)\n",
      "Y_train shape :  (1410, 2)\n",
      "X_test shape :  (157, 224, 224, 3)\n",
      "Y_test shape :  (157, 2)\n",
      "Epoch 1/10\n",
      "1410/1410 [==============================] - 2157s 2s/step - loss: 0.7137 - acc: 0.9241\n",
      "Epoch 2/10\n",
      "1360/1410 [===========================>..] - ETA: 1:20 - loss: 0.2852 - acc: 0.9309"
     ]
    }
   ],
   "source": [
    "all_TP_train=[]\n",
    "all_FP_train=[]\n",
    "all_TN_train=[]\n",
    "all_FN_train=[]\n",
    "all_TP_test=[]\n",
    "all_FP_test=[]\n",
    "all_TN_test=[]\n",
    "all_FN_test=[]\n",
    "\n",
    "nRun = 10\n",
    "side_len = 22\n",
    "num_class = 2\n",
    "for dup_tag in range(2):\n",
    "    for ran_tag in range(2):\n",
    "        for fold in range(nRun):\n",
    "            iFold = fold + 1\n",
    "            X_train,Y_train,X_test,Y_test = DataSet(iFold,dup_tag,ran_tag)\n",
    "            print('X_train shape : ',X_train.shape)\n",
    "            print('Y_train shape : ',Y_train.shape)\n",
    "            print('X_test shape : ',X_test.shape)\n",
    "            print('Y_test shape : ',Y_test.shape)\n",
    "            #print(X_train[0])\n",
    "            #print(Y_train[0,1])\n",
    "\n",
    "            # # model\n",
    "            #model = neural(1,side_len,side_len,num_class)\n",
    "            model = ResNet50(\n",
    "                    weights=None,\n",
    "                    classes=2)\n",
    "            \n",
    "            model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            # # train\n",
    "            model.fit(X_train, Y_train, epochs=10, batch_size=16)\n",
    "\n",
    "            # # evaluate\n",
    "            #model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "            # #\n",
    "            #models.append(model)\n",
    "\n",
    "\n",
    "            # # save\n",
    "            #save_path =(\"./my_resnet_model.h5_F%d\") % (iFold)\n",
    "            #model.save(save_path)\n",
    "\n",
    "            #\n",
    "            X_train,Y_train,X_test,Y_test = DataSet(iFold,0,ran_tag)\n",
    "            train_N_TP,train_N_FP,train_N_TN,train_N_FN = get_statistics(model,X_train,Y_train)\n",
    "            test_N_TP,test_N_FP,test_N_TN,test_N_FN = get_statistics(model,X_test,Y_test)\n",
    "            all_TP_train.append(train_N_TP)\n",
    "            all_FP_train.append(train_N_FP)\n",
    "            all_TN_train.append(train_N_TN)\n",
    "            all_FN_train.append(train_N_FN)\n",
    "            all_TP_test.append(test_N_TP)\n",
    "            all_FP_test.append(test_N_FP)\n",
    "            all_TN_test.append(test_N_TN)\n",
    "            all_FN_test.append(test_N_FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dup_tag in range(2):\n",
    "    for ran_tag in range(2):\n",
    "        for iRun in range(nRun):\n",
    "            print(all_TP_train[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            print(all_FP_train[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            print(all_TN_train[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            print(all_FN_train[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "        for iRun in range(nRun):\n",
    "            print(all_TP_test[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            print(all_FP_test[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            print(all_TN_test[dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            print(all_FN_test[dup_tag*2*nRun+ran_tag*nRun+iRun])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # restore\n",
    "#model = tf.keras.models.load_model('my_resnet_model.h5')\n",
    "\n",
    "# # test\n",
    "#img_path = \"../my_nn/dataset/test/medicine/IMG_20190717_135408_BURST91.jpg\"\n",
    "#img_path = \"../my_nn/dataset/test/glue/IMG_20190717_135425_BURST91.jpg\"\n",
    "\n",
    "#img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "#plt.imshow(img)\n",
    "#img = image.img_to_array(img)/ 255.0\n",
    "#img = np.expand_dims(img, axis=0)  # 为batch添加第四维\n",
    "\n",
    "#print(model.predict(img))\n",
    "#np.argmax(model.predict(img))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
