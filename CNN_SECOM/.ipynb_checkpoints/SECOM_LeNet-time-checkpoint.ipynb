{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\toolkits.win\\Anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\toolkits.win\\Anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "#from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from PIL import Image\n",
    "import random\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dropout, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.applications.mobilenetv2 in keras.applications:\n",
      "\n",
      "NAME\n",
      "    keras.applications.mobilenetv2\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    d:\\toolkits.win\\anaconda3-5.3.1\\envs\\tensorflow-gpu-1.10\\lib\\site-packages\\keras\\applications\\mobilenetv2.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(keras.applications.mobilenetv2)\n",
    "help(keras.applications.mobilenetv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2array(path, delimiter):\n",
    "    recordlist = []\n",
    "    fp = open(path, 'r', encoding='utf-8')\n",
    "    content = fp.read()     # content现在是一行字符串，该字符串包含文件所有内容\n",
    "    fp.close()\n",
    "    rowlist = content.splitlines()  # 按行转换为一维表，splitlines默认参数是‘\\n’\n",
    "    # 逐行遍历\n",
    "    # 结果按分隔符分割为行向量\n",
    "    recordlist = [row.split(delimiter) for row in rowlist if row.strip()]\n",
    "    return np.array(recordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "def ndarray_zoom_scaling(label, new_h, new_w):\n",
    "    \"\"\"\n",
    "    Implement scaling for ndarray with scipy.ndimage.zoom\n",
    "    :param label: [H, W] or [H, W, C]\n",
    "    :return: label_new: [new_h, new_w] or [new_h, new_w, C]\n",
    "    Examples\n",
    "    --------\n",
    "    ori_arr = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]], dtype=np.int32)\n",
    "    new_arr = ndarray_zoom_scaling(ori_arr, new_h=8, new_w=10)\n",
    "    >> print(new_arr)\n",
    "    [[1 1 1 2 2 2 2 3 3 3]\n",
    "     [1 1 1 2 2 2 2 3 3 3]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [4 4 4 5 5 5 5 6 6 6]\n",
    "     [7 7 7 8 8 8 8 9 9 9]\n",
    "     [7 7 7 8 8 8 8 9 9 9]]\n",
    "    \"\"\"\n",
    "    scale_h = new_h / label.shape[0]\n",
    "    scale_w = new_w / label.shape[1]\n",
    "    if len(label.shape) == 2:\n",
    "        label_new = scipy.ndimage.zoom(label, zoom=[scale_h, scale_w], order=0)\n",
    "    else:\n",
    "        label_new = scipy.ndimage.zoom(label, zoom=[scale_h, scale_w, 1], order=0)\n",
    "    return label_new\n",
    "\n",
    "\n",
    "def ndarray_nearest_neighbour_scaling(label, new_h, new_w):\n",
    "    \"\"\"\n",
    "    Implement nearest neighbour scaling for ndarray\n",
    "    :param label: [H, W] or [H, W, C]\n",
    "    :return: label_new: [new_h, new_w] or [new_h, new_w, C]\n",
    "    Examples\n",
    "    --------\n",
    "    ori_arr = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]], dtype=np.int32)\n",
    "    new_arr = ndarray_nearest_neighbour_scaling(ori_arr, new_h=8, new_w=10)\n",
    "    >> print(new_arr)\n",
    "    [[1 1 1 1 2 2 2 3 3 3]\n",
    "     [1 1 1 1 2 2 2 3 3 3]\n",
    "     [1 1 1 1 2 2 2 3 3 3]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [4 4 4 4 5 5 5 6 6 6]\n",
    "     [7 7 7 7 8 8 8 9 9 9]\n",
    "     [7 7 7 7 8 8 8 9 9 9]]\n",
    "    \"\"\"\n",
    "#    if len(label.shape) == 2:\n",
    "#        label_new = np.zeros([new_h, new_w], dtype=label.dtype)\n",
    "#    else:\n",
    "#        label_new = np.zeros([new_h, new_w, label.shape[2]], dtype=label.dtype)\n",
    "    label_new = np.zeros([new_h, new_w, label.shape[2]], dtype=label.dtype)\n",
    "\n",
    "    scale_h = new_h / label.shape[0]\n",
    "    scale_w = new_w / label.shape[1]\n",
    "\n",
    "    y_pos = np.arange(new_h)\n",
    "    x_pos = np.arange(new_w)\n",
    "    y_pos = np.floor(y_pos / scale_h).astype(np.int32)\n",
    "    x_pos = np.floor(x_pos / scale_w).astype(np.int32)\n",
    "\n",
    "    y_pos = y_pos.reshape(y_pos.shape[0], 1)\n",
    "    y_pos = np.tile(y_pos, (1, new_w))\n",
    "    x_pos = np.tile(x_pos, (new_h, 1))\n",
    "    assert y_pos.shape == x_pos.shape\n",
    "\n",
    "    label_new[:, :] = label[y_pos[:, :], x_pos[:, :]]\n",
    "    return label_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataSet(iFold,noise_tag,dup_tag,rand_tag):\n",
    "    #\n",
    "    train_path =(\"../Data_CNN_Indus/F%d/SECOM_samples_train_F%d\") % (iFold, iFold)    \n",
    "    test_path  =(\"../Data_CNN_Indus/F%d/SECOM_samples_test_F%d\") % (iFold, iFold)\n",
    "    #\n",
    "    data_train = file2array(train_path, \" \")\n",
    "    nrow_train = data_train.shape[0]\n",
    "    ncol_train = data_train.shape[1]\n",
    "    #\n",
    "    data_test  = file2array(test_path, \" \")\n",
    "    nrow_test  = data_test.shape[0]\n",
    "    ncol_test  = data_test.shape[1]\n",
    "    #\n",
    "    if ncol_train != ncol_test:\n",
    "        print('Numbers of features are not consistant.')\n",
    "    if nrow_train + nrow_test != 1567:\n",
    "        print('Numbers of samples are not consistant.')\n",
    "    #\n",
    "    train_path =(\"../Data_CNN_Indus/F%d/SECOM_labels_train_F%d\") % (iFold, iFold)    \n",
    "    test_path  =(\"../Data_CNN_Indus/F%d/SECOM_labels_test_F%d\") % (iFold, iFold)\n",
    "    #\n",
    "    lab_train = file2array(train_path, \" \")\n",
    "    #\n",
    "    lab_test  = file2array(test_path, \" \")\n",
    "    #\n",
    "    num_feature = ncol_train\n",
    "    side_len  = math.ceil(math.sqrt(num_feature))\n",
    "    num_class = 2\n",
    "    #\n",
    "    mat_ind_path =(\"../Data_CNN_Indus/Mat_ind_F%d.txt\") % (iFold)\n",
    "    #\n",
    "    mat_ind = file2array(mat_ind_path, \"\\t\")\n",
    "    \n",
    "    if (int)(rand_tag):\n",
    "        tmp_vec = np.zeros(num_feature)\n",
    "        for index in range(num_feature):\n",
    "            r = (int)(index/side_len)\n",
    "            c = (int)(index%side_len)\n",
    "            tmp_vec[index] = int(mat_ind[r,c])\n",
    "        index = [i for i in range(num_feature)]\n",
    "        random.shuffle(index)\n",
    "        tmp_vec = tmp_vec[index]\n",
    "        for index in range(num_feature):\n",
    "            r = (int)(index/side_len)\n",
    "            c = (int)(index%side_len)\n",
    "            mat_ind[r,c] = int(tmp_vec[index])\n",
    "\n",
    "    tmp_cnt_per_class = np.zeros(num_class)\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        tmp_cnt_per_class[(int)(tmp)] = tmp_cnt_per_class[(int)(tmp)] + 1\n",
    "        \n",
    "    tmp_max = tmp_cnt_per_class[0]\n",
    "    tmp_ind = 0\n",
    "    for index in range(num_class):\n",
    "        if tmp_cnt_per_class[index] > tmp_max:\n",
    "            tmp_max = tmp_cnt_per_class[index]\n",
    "            tmp_ind = index\n",
    "            \n",
    "    num_dup = np.zeros(num_class)\n",
    "    for index in range(num_class):\n",
    "        if (int)(dup_tag):\n",
    "            num_dup[index] = math.floor(tmp_cnt_per_class[tmp_ind]/tmp_cnt_per_class[index])\n",
    "        else:\n",
    "            num_dup[index] = 1\n",
    "        \n",
    "    nrow_train_dup = 0\n",
    "    for index in range(num_class):\n",
    "        nrow_train_dup = nrow_train_dup + tmp_cnt_per_class[index] * num_dup[index]\n",
    "        \n",
    "    nrow_train_dup = (int)(nrow_train_dup)\n",
    "    \n",
    "    X_train = np.empty((nrow_train_dup, side_len, side_len, 1), dtype=float)\n",
    "    Y_train = np.empty((nrow_train_dup, num_class), dtype=float)\n",
    "\n",
    "    sam_count = 0\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        nDup = num_dup[(int)(tmp)]\n",
    "        nDup = (int)(nDup)\n",
    "        for iSam in range(nDup):\n",
    "            tmp_cnt = 0\n",
    "            for i in range(side_len):\n",
    "                for j in range(side_len):\n",
    "                    if tmp_cnt < num_feature:\n",
    "                        tmp_ind = mat_ind[i,j]\n",
    "                        X_train[sam_count,i,j,0] = float(data_train[index,(int)(tmp_ind)])\n",
    "                    else:\n",
    "                        X_train[sam_count,i,j,0] = 0.5\n",
    "                    tmp_cnt = tmp_cnt + 1\n",
    "            sam_count = sam_count + 1\n",
    "\n",
    "    sam_count = 0\n",
    "    for index in range(nrow_train):\n",
    "        tmp = lab_train[index,0]\n",
    "        nDup = num_dup[(int)(tmp)]\n",
    "        nDup = (int)(nDup)\n",
    "        for iSam in range(nDup):\n",
    "            tmp_v = np.zeros(num_class)\n",
    "            tmp_v[(int)(tmp)] = 1\n",
    "            Y_train[sam_count] = tmp_v\n",
    "            sam_count = sam_count + 1\n",
    "    \n",
    "    X_test  = np.empty((nrow_test, side_len, side_len, 1), dtype=float)\n",
    "    Y_test  = np.empty((nrow_test, num_class), dtype=float)\n",
    "\n",
    "    for index in range(nrow_test):\n",
    "        tmp_cnt = 0\n",
    "        for i in range(side_len):\n",
    "            for j in range(side_len):\n",
    "                if tmp_cnt < num_feature:\n",
    "                    tmp_ind = mat_ind[i,j]\n",
    "                    tmp_var = float(data_test[index,(int)(tmp_ind)])\n",
    "                    #print(tmp_var)\n",
    "                    #if tmp_var < 0:\n",
    "                    #    tmp_var = 0\n",
    "                    #if tmp_var > 1:\n",
    "                    #    tmp_var = 1\n",
    "                    X_test[index,i,j,0] = tmp_var\n",
    "                else:\n",
    "                    X_test[index,i,j,0] = 0.5\n",
    "                tmp_cnt = tmp_cnt + 1\n",
    "    \n",
    "    for index in range(nrow_test):\n",
    "        tmp = lab_test[index,0]\n",
    "        tmp_v = np.zeros(num_class)\n",
    "        tmp_v[(int)(tmp)] = 1\n",
    "        Y_test[index] = tmp_v\n",
    "        \n",
    "    if (int)(noise_tag):\n",
    "        noise = np.random.normal(loc=0,scale=1e-2,size=X_train.shape)\n",
    "        output = X_train + noise\n",
    "        X_train = np.concatenate((X_train, output), axis=0)\n",
    "        Y_train = np.concatenate((Y_train, Y_train), axis=0)\n",
    "    \n",
    "    index = [i for i in range(len(X_train))]\n",
    "    random.shuffle(index)\n",
    "    X_train = X_train[index]\n",
    "    Y_train = Y_train[index]\n",
    "    \n",
    "    index = [i for i in range(len(X_test))]\n",
    "    random.shuffle(index)\n",
    "    X_test = X_test[index]    \n",
    "    Y_test = Y_test[index]\n",
    "    \n",
    "#    X_train_new = np.empty((len(X_train), 224, 224, 3))\n",
    "#    #Y_train_new = Y_train\n",
    "#    \n",
    "#    for index in range(len(X_train)):\n",
    "#        tmp_new = ndarray_nearest_neighbour_scaling(X_train[index],224,224)\n",
    "#        for c in range(3):\n",
    "#            X_train_new[index,:,:,c] = tmp_new[:,:,0]\n",
    "#    \n",
    "#    X_test_new = np.empty((len(X_test), 224, 224, 3))\n",
    "#    #Y_test_new = Y_test\n",
    "#    \n",
    "#    for index in range(len(X_test)):\n",
    "#        tmp_new = ndarray_nearest_neighbour_scaling(X_test[index],224,224)\n",
    "#        for c in range(3):\n",
    "#            X_test_new[index,:,:,c] = tmp_new[:,:,0]\n",
    "#            \n",
    "#    del X_train, X_test\n",
    "#    gc.collect()\n",
    "\n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "def LeakyRelu(x, leak=0.01, name=\"LeakyRelu\"):\n",
    "   with tf.variable_scope(name):\n",
    "     f1 = 0.5 * (1 + leak)\n",
    "     f2 = 0.5 * (1 - leak)\n",
    "     return f1 * x + f2 * tf.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(channel,height,width,classes):\n",
    "    input_shape = (channel,height,width)\n",
    "    if K.image_data_format() == \"channels_last\":#确认输入维度,就是channel是在开头，还是结尾\n",
    "        input_shape = (height,width,channel)\n",
    "    model = Sequential()#顺序模型（keras中包括顺序模型和函数式API两种方式）\n",
    "    model.add(Conv2D(6,(3,3),padding=\"valid\",activation=LeakyRelu,input_shape=input_shape,name=\"conv1\"))\n",
    "    #model.add(Conv2D(6,(3,3),padding=\"valid\",input_shape=input_shape,name=\"conv1\"))\n",
    "    #model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2),name=\"pool1\"))\n",
    "    model.add(Conv2D(12,(3,3),padding=\"valid\",activation=LeakyRelu,name=\"conv2\",))\n",
    "    #model.add(Conv2D(12,(3,3),padding=\"valid\",name=\"conv2\",))\n",
    "    #model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2),name=\"pool2\"))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(192,activation=\"relu\",name=\"fc1\"))\n",
    "    model.add(Dense(classes,activation=\"softmax\",name=\"fc2\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(model,sam_data,lab_data):\n",
    "    num_class = 2\n",
    "\n",
    "    #\n",
    "    test_rows = len(lab_data)\n",
    "    test_prc = np.zeros(num_class)\n",
    "    test_rec = np.zeros(num_class)\n",
    "    test_Fva = np.zeros(num_class)\n",
    "    test_BER = np.zeros(num_class)\n",
    "    tmp_beta = 1\n",
    "    yhat_out    = model.predict(sam_data, batch_size=32, verbose=1)\n",
    "    yhat_label  = np.zeros(len(lab_data))\n",
    "    true_labels = np.zeros(len(lab_data))\n",
    "    for index in range(len(lab_data)):\n",
    "        tmp_vec = lab_data[index]\n",
    "        tmp_max = tmp_vec[0]\n",
    "        tmp_ind = 0\n",
    "        for i in range(num_class):\n",
    "            if tmp_max < tmp_vec[i]:\n",
    "                tmp_max = tmp_vec[i]\n",
    "                tmp_ind = i\n",
    "        true_labels[index] = tmp_ind\n",
    "        #\n",
    "        tmp_vec = yhat_out[index]\n",
    "        tmp_max = tmp_vec[0]\n",
    "        tmp_ind = 0\n",
    "        for i in range(num_class):\n",
    "            if tmp_max < tmp_vec[i]:\n",
    "                tmp_max = tmp_vec[i]\n",
    "                tmp_ind = i\n",
    "        yhat_label[index] = tmp_ind\n",
    "\n",
    "    test_N_TP = np.zeros(num_class)\n",
    "    test_N_FP = np.zeros(num_class)\n",
    "    test_N_TN = np.zeros(num_class)\n",
    "    test_N_FN = np.zeros(num_class)\n",
    "    tmp_count = np.zeros(num_class)\n",
    "    for i in range(test_rows):\n",
    "        cur_label = yhat_label[i]\n",
    "        true_label = true_labels[i]\n",
    "        for j in range(num_class):\n",
    "            if j == cur_label:\n",
    "                if j == true_label:\n",
    "                    test_N_TP[j]=test_N_TP[j]+1\n",
    "                    tmp_count[j] = tmp_count[j] + 1\n",
    "                else:\n",
    "                    test_N_FP[j]=test_N_FP[j]+1\n",
    "            else:\n",
    "                if j == true_label:\n",
    "                    test_N_FN[j]=test_N_FN[j]+1\n",
    "                    tmp_count[j] = tmp_count[j] + 1\n",
    "                else:\n",
    "                    test_N_TN[j]=test_N_TN[j]+1\n",
    "\n",
    "    for i in range(num_class):\n",
    "        if test_N_TP[i] > 0:\n",
    "            test_prc[i] = test_N_TP[i] / (test_N_TP[i] + test_N_FP[i])\n",
    "        else:\n",
    "            test_prc[i] = 0\n",
    "\n",
    "        test_rec[i]= test_N_TP[i] / (test_N_TP[i] + test_N_FN[i])\n",
    "        test_Fva[i]=(1 + tmp_beta*tmp_beta)*test_rec[i]*test_prc[i]/\\\n",
    "                      (tmp_beta*tmp_beta*(test_rec[i]+test_prc[i]))\n",
    "        test_BER[i]= test_N_FN[i] / (test_N_TP[i] + test_N_FN[i])\n",
    "        \n",
    "    print(tmp_count)\n",
    "        \n",
    "    return test_N_TP,test_N_FP,test_N_TN,test_N_FN,tmp_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (1410, 22, 22, 1)\n",
      "Y_train shape :  (1410, 2)\n",
      "X_test shape :  (157, 22, 22, 1)\n",
      "Y_test shape :  (157, 2)\n",
      "0 0 0 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-48cb881d7825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# # train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mall_time_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "all_TP_train=[]\n",
    "all_FP_train=[]\n",
    "all_TN_train=[]\n",
    "all_FN_train=[]\n",
    "all_count_train=[]\n",
    "all_TP_test=[]\n",
    "all_FP_test=[]\n",
    "all_TN_test=[]\n",
    "all_FN_test=[]\n",
    "all_count_test=[]\n",
    "all_time_train=[]\n",
    "all_time_predict_train=[]\n",
    "all_time_predict_test=[]\n",
    "\n",
    "nRun = 10\n",
    "side_len = 22\n",
    "num_class = 2\n",
    "for noise_tag in range(2):\n",
    "    for dup_tag in range(2):\n",
    "        for ran_tag in range(2):\n",
    "            for fold in range(nRun):\n",
    "                iFold = fold + 1\n",
    "                X_train,Y_train,X_test,Y_test = DataSet(iFold,noise_tag,dup_tag,ran_tag)\n",
    "                print('X_train shape : ',X_train.shape)\n",
    "                print('Y_train shape : ',Y_train.shape)\n",
    "                print('X_test shape : ',X_test.shape)\n",
    "                print('Y_test shape : ',Y_test.shape)\n",
    "                #print(X_train[0])\n",
    "                #print(Y_train[0,1])\n",
    "                print(noise_tag,dup_tag,ran_tag,iFold)\n",
    "\n",
    "                # # model\n",
    "                #model = tf.keras.applications.mobilenet.MobileNet(\n",
    "                #    input_shape=None, \n",
    "                #    alpha=1.0, \n",
    "                #    depth_multiplier=1, \n",
    "                #    include_top=True, \n",
    "                #    weights=None, \n",
    "                #    input_tensor=None, \n",
    "                #    pooling='max', \n",
    "                #    classes=2\n",
    "                #)\n",
    "\n",
    "                ##\n",
    "                model = neural(1,side_len,side_len,num_class)\n",
    "\n",
    "                model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "                # # train\n",
    "                start = time.clock()\n",
    "                model.fit(X_train, Y_train, epochs=10, batch_size=16)\n",
    "                all_time_train.append(time.clock()-start)\n",
    "\n",
    "                # # evaluate\n",
    "                #model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "                # #\n",
    "                #models.append(model)\n",
    "\n",
    "                # # save\n",
    "                #save_path =(\"./my_resnet_model.h5_F%d\") % (iFold)\n",
    "                #model.save(save_path)\n",
    "\n",
    "                #\n",
    "                X_train,Y_train,X_test,Y_test = DataSet(iFold,0,0,ran_tag)\n",
    "                start = time.clock()\n",
    "                train_N_TP,train_N_FP,train_N_TN,train_N_FN,train_count = get_statistics(model,X_train,Y_train)\n",
    "                all_time_predict_train.append(time.clock()-start)\n",
    "                start = time.clock()\n",
    "                test_N_TP,test_N_FP,test_N_TN,test_N_FN,test_count = get_statistics(model,X_test,Y_test)\n",
    "                all_time_predict_test.append(time.clock()-start)\n",
    "                all_TP_train.append(train_N_TP)\n",
    "                all_FP_train.append(train_N_FP)\n",
    "                all_TN_train.append(train_N_TN)\n",
    "                all_FN_train.append(train_N_FN)\n",
    "                all_count_train.append(train_count)\n",
    "                all_TP_test.append(test_N_TP)\n",
    "                all_FP_test.append(test_N_FP)\n",
    "                all_TN_test.append(test_N_TN)\n",
    "                all_FN_test.append(test_N_FN)\n",
    "                all_count_test.append(test_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(loc=0,scale=1e-2,size=X_train.shape)\n",
    "output = X_train + noise\n",
    "X_train_all = np.concatenate((X_train, output), axis=0)\n",
    "print(noise)\n",
    "print(noise.shape)\n",
    "print(output.shape)\n",
    "print(X_train.shape)\n",
    "print(X_train_all.shape)\n",
    "for i in range(len(X_train)):\n",
    "    if (X_train[i]==X_train_all[i]).all()==False:\n",
    "        print(i)\n",
    "for i in range(len(X_train)):\n",
    "    ind = len(X_train)+i\n",
    "    if (output[i]==X_train_all[ind]).all()==False:\n",
    "        print(ind)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for noise_tag in range(2):\n",
    "    for dup_tag in range(2):\n",
    "        for ran_tag in range(2):\n",
    "            for iRun in range(nRun):\n",
    "                print(all_TP_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_FP_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_TN_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_FN_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "            for iRun in range(nRun):\n",
    "                print(all_TP_test[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_FP_test[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_TN_test[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_FN_test[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "\n",
    "for noise_tag in range(2):\n",
    "    for dup_tag in range(2):\n",
    "        for ran_tag in range(2):\n",
    "            for iRun in range(nRun):\n",
    "                print(all_count_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n",
    "                print(all_count_test[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_tag in range(2):\n",
    "    for dup_tag in range(2):\n",
    "        for ran_tag in range(2):\n",
    "            for iRun in range(nRun):\n",
    "                print(all_time_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_tag in range(2):\n",
    "    for dup_tag in range(2):\n",
    "        for ran_tag in range(2):\n",
    "            for iRun in range(nRun):\n",
    "                print(all_time_predict_train[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_tag in range(2):\n",
    "    for dup_tag in range(2):\n",
    "        for ran_tag in range(2):\n",
    "            for iRun in range(nRun):\n",
    "                print(all_time_predict_test[noise_tag*2*2*nRun+dup_tag*2*nRun+ran_tag*nRun+iRun])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # restore\n",
    "#model = tf.keras.models.load_model('my_resnet_model.h5')\n",
    "\n",
    "# # test\n",
    "#img_path = \"../my_nn/dataset/test/medicine/IMG_20190717_135408_BURST91.jpg\"\n",
    "#img_path = \"../my_nn/dataset/test/glue/IMG_20190717_135425_BURST91.jpg\"\n",
    "\n",
    "#img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "#plt.imshow(img)\n",
    "#img = image.img_to_array(img)/ 255.0\n",
    "#img = np.expand_dims(img, axis=0)  # 为batch添加第四维\n",
    "\n",
    "#print(model.predict(img))\n",
    "#np.argmax(model.predict(img))\n",
    "\n",
    "#save_path =(\"./tmp_TP_EP%d\") % (1)\n",
    "#fl=open(save_path, 'w')\n",
    "#lists=[line+\"\\n\" for line in all_TP_train]\n",
    "#fl.writelines(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
